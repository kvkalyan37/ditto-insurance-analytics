name: Scheduled Data Update (Every 30 Minutes)

on:
  schedule:
    # Run every 30 minutes
    - cron: '*/30 * * * *'
  workflow_dispatch: # Allow manual triggering

env:
  DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
  FRONTEND_IMAGE: ${{ secrets.DOCKERHUB_USERNAME }}/ditto-frontend
  API_IMAGE: ${{ secrets.DOCKERHUB_USERNAME }}/ditto-api

jobs:
  scrape-and-update:
    name: Scrape Data and Update Deployment
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          pip install -r requirements_scraper.txt

      - name: Run scraper to fetch latest data
        id: scrape
        run: |
          echo "ðŸ•·ï¸ Starting data scrape from Ditto..."
          python scrape_ditto.py --output ditto_insurance_data.csv
          
          # Check if data file was created and has content
          if [ ! -f ditto_insurance_data.csv ]; then
            echo "âŒ Error: Data file not created"
            exit 1
          fi
          
          # Count records
          RECORD_COUNT=$(tail -n +2 ditto_insurance_data.csv | wc -l | tr -d ' ')
          echo "ðŸ“Š Records scraped: $RECORD_COUNT"
          echo "record_count=$RECORD_COUNT" >> $GITHUB_OUTPUT
          
          # Calculate file hash to detect changes
          FILE_HASH=$(sha256sum ditto_insurance_data.csv | cut -d' ' -f1)
          echo "file_hash=$FILE_HASH" >> $GITHUB_OUTPUT
          echo "ðŸ“ Data file hash: $FILE_HASH"

      - name: Get previous data hash
        id: get_prev_hash
        uses: actions/cache@v4
        with:
          path: .data_hash
          key: ditto-data-hash-${{ github.ref }}
          restore-keys: |
            ditto-data-hash-

      - name: Check for data changes
        id: check_changes
        run: |
          # Get previous hash from cache
          PREVIOUS_HASH=""
          if [ -f .data_hash ]; then
            PREVIOUS_HASH=$(cat .data_hash)
            echo "ðŸ“‹ Previous data hash: $PREVIOUS_HASH"
          else
            echo "ðŸ“‹ No previous hash found (first run or cache miss)"
          fi
          
          CURRENT_HASH="${{ steps.scrape.outputs.file_hash }}"
          echo "ðŸ“‹ Current data hash: $CURRENT_HASH"
          
          if [ "$PREVIOUS_HASH" == "$CURRENT_HASH" ]; then
            echo "â„¹ï¸ No data changes detected. Skipping build and deployment."
            echo "data_changed=false" >> $GITHUB_OUTPUT
          else
            echo "âœ… Data changed! New hash: $CURRENT_HASH"
            echo "data_changed=true" >> $GITHUB_OUTPUT
            # Save current hash for next run
            echo "$CURRENT_HASH" > .data_hash
          fi

      - name: Save data hash to cache
        if: steps.check_changes.outputs.data_changed == 'true'
        uses: actions/cache@v4
        with:
          path: .data_hash
          key: ditto-data-hash-${{ github.ref }}-${{ steps.scrape.outputs.file_hash }}

      - name: Set up Docker Buildx
        if: steps.check_changes.outputs.data_changed == 'true'
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        if: steps.check_changes.outputs.data_changed == 'true'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Generate timestamp tag
        if: steps.check_changes.outputs.data_changed == 'true'
        id: timestamp
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          echo "timestamp=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "ðŸ• Generated timestamp: $TIMESTAMP"

      - name: Copy data to API image context
        if: steps.check_changes.outputs.data_changed == 'true'
        run: |
          # Copy CSV data to api_service directory for Docker build
          mkdir -p api_service/data
          cp ditto_insurance_data.csv api_service/data/
          echo "âœ… Data file copied to API build context"

      - name: Build and push Frontend image with timestamp
        if: steps.check_changes.outputs.data_changed == 'true'
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          platforms: linux/amd64
          push: true
          tags: ${{ env.FRONTEND_IMAGE }}:${{ steps.timestamp.outputs.timestamp }}
          cache-to: type=inline
          no-cache-filters: true

      - name: Build and push API image with timestamp (includes new data)
        if: steps.check_changes.outputs.data_changed == 'true'
        uses: docker/build-push-action@v5
        with:
          context: ./api_service
          file: ./api_service/Dockerfile
          platforms: linux/amd64
          push: true
          tags: ${{ env.API_IMAGE }}:${{ steps.timestamp.outputs.timestamp }}
          cache-to: type=inline
          no-cache-filters: true

      - name: Upload data file as artifact
        if: steps.check_changes.outputs.data_changed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: ditto-insurance-data
          path: ditto_insurance_data.csv
          retention-days: 7
          if-no-files-found: error

      - name: Final Summary
        run: |
          if [ "${{ steps.check_changes.outputs.data_changed }}" == "true" ]; then
            echo "## âœ… Scheduled Update Complete" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“Š Scraping Results" >> $GITHUB_STEP_SUMMARY
            echo "- **Records scraped**: ${{ steps.scrape.outputs.record_count }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Data hash**: \`${{ steps.scrape.outputs.file_hash }}\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ³ Docker Images Built" >> $GITHUB_STEP_SUMMARY
            echo "- **Frontend**: \`${{ env.FRONTEND_IMAGE }}:${{ steps.timestamp.outputs.timestamp }}\`" >> $GITHUB_STEP_SUMMARY
            echo "- **API**: \`${{ env.API_IMAGE }}:${{ steps.timestamp.outputs.timestamp }}\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“¥ Available on Docker Hub" >> $GITHUB_STEP_SUMMARY
            echo "Images are now available on Docker Hub and ready to deploy!" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸš€ Next Steps" >> $GITHUB_STEP_SUMMARY
            echo "1. Pull the new images from Docker Hub" >> $GITHUB_STEP_SUMMARY
            echo "2. Deploy to your hosting platform (Railway, Render, etc.)" >> $GITHUB_STEP_SUMMARY
            echo "3. Or configure auto-deploy from Docker Hub" >> $GITHUB_STEP_SUMMARY
          else
            echo "## â„¹ï¸ No Data Changes Detected" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“Š Scraping Results" >> $GITHUB_STEP_SUMMARY
            echo "- **Records scraped**: ${{ steps.scrape.outputs.record_count }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Data hash**: \`${{ steps.scrape.outputs.file_hash }}\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### â­ï¸ Status" >> $GITHUB_STEP_SUMMARY
            echo "No changes detected. Skipped image build and deployment." >> $GITHUB_STEP_SUMMARY
            echo "Next check in 30 minutes." >> $GITHUB_STEP_SUMMARY
          fi
